# Метод стохастического градиентного спуска (SGD) и его модификации

## Основное задание
Реализуйте и исследуйте на эффективность SGD для решения линейной
регрессии:
1. с разным размером батча – от одного до размера полной коллекции
(обычный GD);
2. с разной функцией изменения шага (learning rate scheduling), например,
экспоненциальной или ступенчатой.
3. scipy.optimize: SGD, и модификации SGD (Nesterov, Momentum, AdaGrad,
RMSProp, Adam). Изучите параметры вызываемых библиотечных
функций.

## Описание метода
В описании метода указывайте из какой библиотеки он взят, или какую
библиотеку использует внутри реализации. Также, по пути собирайте
hyperпараметры (они могут пригодиться потом) – то есть указывайте, какие
гиперпараметры есть у метода – их следует выносить в интерфейс метода.
Указывайте, с какими значениями гиперпараметров были получены результаты.

## Содержание исследования
Для исследования можно генерировать данные (и, тем самым, оценивать,
насколько хорошо метод решает задачу восстановления регрессии), можно
выбрать данные из архива https://archive.ics.uci.edu/ или любых других
ресурсов.
Сравните эффективность SGD с разными параметрами и его модификации по
точности, скорости и ресурсам: объёму требуемой оперативной памяти и
количеству арифметических операций. Иллюстрируйте примеры, чтобы не
было скучно.

## Дополнительное задание 1
Реализуйте и исследуйте на эффективность SGD для полиномиальной
регрессии с добавлением регуляризации в модель разных методов
регуляризации (L1, L2, Elastic регуляризации).

## Дополнительное задание 2
Разберите подробней постановку задачи оптимизации в одной из задач
машинного обучения (метод опорных векторов, активное обучение, бустинг и
пр.). Приведите пример, иллюстрирующий задачу и её решение (найдите на
ресурсах).